

***

### **Transformer 训练超参数符号系统与维度定义**




#### **一、优化器参数**



| **符号**                     | **描述**               | **维度** | **数学定义 / 作用**                                                                                                  |
| -------------------------- | -------------------- | ------ | -------------------------------------------------------------------------------------------------------------- |
| $\alpha$                   | 基础学习率（Learning Rate） | 标量     | 参数更新步长，如 $\theta_{t + 1} = \theta_t - \alpha \cdot g_t$                                                        |
| $\beta_1$                  | Adam 优化器一阶矩衰减率       | 标量     | 动量衰减系数，默认 0.9，用于计算移动平均梯度 $m_t = \beta_1 m_{t - 1} + (1 - \beta_1)g_t$                                          |
| $\beta_2$                  | Adam 优化器二阶矩衰减率       | 标量     | 自适应学习率衰减系数，默认 0.999，计算梯度平方移动平均 $v_t = \beta_2 v_{t - 1} + (1 - \beta_2)g_t^2$                                  |
| $\epsilon$                 | Adam 优化器数值稳定项        | 标量     | 防止除零，默认 1e - 8，用于参数更新 $\theta_{t + 1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$ |
| $\lambda$                  | 权重衰减系数（Weight Decay） | 标量     | L2 正则化强度，更新公式 $\theta_{t + 1} = \theta_t - \alpha \cdot (g_t + \lambda \theta_t)$                              |
| $\gamma_{\text{momentum}}$ | SGD 动量系数             | 标量     | 传统动量项，更新公式 $v_t = \gamma v_{t - 1} + g_t$, $\theta_{t + 1} = \theta_t - \alpha v_t$                            |





#### **二、正则化参数**



| **符号**                 | **描述**                    | **维度** | **数学定义 / 作用**                                                             |
| ---------------------- | ------------------------- | ------ | ------------------------------------------------------------------------- |
| $p_{\text{drop}}$      | Dropout 丢弃概率              | 标量     | 激活值按概率置零，如 $x_{\text{drop}} = \text{Dropout}(x, p_{\text{drop}})$         |
| $\epsilon_{\text{ls}}$ | 标签平滑系数（Label Smoothing）   | 标量     | 软化目标分布，将真实标签概率设为 \$1 - \epsilon\_{\text {ls}} + \epsilon\_{\text {ls}} /  |
| $\tau$                 | 梯度裁剪阈值（Gradient Clipping） | 标量     | 限制梯度范数，$g_{\text{clip}} = g \cdot \min\left(1, \frac{\tau}{\|g\|}\right)$ |





#### **三、学习率调度参数**



| **符号**                | **描述**      | **维度** | **数学定义 / 作用**                                                                                                              |
| --------------------- | ----------- | ------ | -------------------------------------------------------------------------------------------------------------------------- |
| $T_{\text{warmup}}$   | 学习率预热步数     | 标量     | 初始阶段线性增加学习率至 $\alpha$，避免早期不稳定                                                                                              |
| $\alpha_{\text{max}}$ | 最大学习率（余弦退火） | 标量     | 调度周期内的峰值学习率，如 $\alpha_t = \alpha_{\text{min}} + \frac{1}{2}(\alpha_{\text{max}} - \alpha_{\text{min}})(1 + \cos(\pi t/T))$ |
| $\alpha_{\text{min}}$ | 最小学习率（余弦退火） | 标量     | 调度周期内的谷值学习率                                                                                                                |
| $T_{\text{cycle}}$    | 学习率调度周期长度   | 标量     | 余弦退火或循环策略的周期步数                                                                                                             |





#### **四、训练配置参数**



| **符号**             | **描述**           | **维度** | **数学定义 / 作用**                                                         |
| ------------------ | ---------------- | ------ | --------------------------------------------------------------------- |
| $B$                | 批量大小（Batch Size） | 标量     | 单次参数更新使用的样本数                                                          |
| $T_{\text{total}}$ | 总训练步数            | 标量     | 模型参数更新的总次数                                                            |
| $N_{\text{epoch}}$ | 训练轮数（Epoch）      | 标量     | 完整遍历数据集的次数，\$N\_{\text {epoch}} = \lceil T\_{\text {total}} \cdot B / |




#### **五、参数更新公式示例（Adam 优化器）**

**计算梯度**：

$g_t = \nabla_{\theta} \mathcal{L}(\theta_t)$

**更新一阶矩估计**：

 $m_t= \beta_1 m_{t - 1} + (1 - \beta_1) g_t$

**更新二阶矩估计**：

  $v_t = \beta_2 v_{t - 1} + (1 - \beta_2) g_t^2$

**修正偏差**：

$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$

**参数更新**：

$\theta_{t + 1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \alpha \lambda \theta_t$




#### **六、参数初始化与训练流程**

**初始化优化器状态**：

$m_0 = 0$, $v_0 = 0$（Adam 初始状态）

学习率按预热策略调整：

$\alpha_t = \alpha \cdot \min\left(1, \frac{t}{T_{\text{warmup}}}\right) \quad (0 < t \leq T_{\text{warmup}})$

**单步训练伪代码**：



```
for t in 1...T\_total:

&#x20;   \# 1. 前向传播与损失计算

&#x20;   X, Y = sample\_batch(data, B)

&#x20;   Y\_pred = transformer(X)

&#x20;   loss = cross\_entropy(Y\_pred, Y) + λ \* L2\_norm(θ)

&#x20;  &#x20;

&#x20;   \# 2. 反向传播

&#x20;   gradients = compute\_gradients(loss, θ)

&#x20;  &#x20;

&#x20;   \# 3. 梯度裁剪

&#x20;   gradients = clip\_by\_norm(gradients, τ)

&#x20;  &#x20;

&#x20;   \# 4. 更新参数（Adam）

&#x20;   m = β1 \* m\_prev + (1 - β1) \* gradients

&#x20;   v = β2 \* v\_prev + (1 - β2) \* gradients\*\*2

&#x20;   m\_hat = m / (1 - β1\*\*t)

&#x20;   v\_hat = v / (1 - β2\*\*t)

&#x20;   θ = θ - α\_t \* (m\_hat / (sqrt(v\_hat) + ε) + λ \* θ)
```


### **总结**

此符号系统完整覆盖了 Transformer 模型的 **可调模型参数**（如权重矩阵、归一化参数）与 **训练超参数**（如学习率、优化器配置、正则化项），并通过数学公式与伪代码明确了各参数在训练流程中的作用。关键设计包括：

**模型 - 训练解耦**：区分架构参数（$W, \gamma, \beta$）与训练参数（$\alpha, \beta_1, \lambda$）。

**动态学习率**：通过预热、余弦退火等策略实现精细控制。

**正则化协同**：组合使用 Dropout、标签平滑与梯度裁剪提升泛化性。

此框架为理论分析、超参数调优及代码实现提供了完备的数学参考。



***

### **Transformer 训练超参数符号系统与维度定义（详细版）**

#### **一、优化器参数**



| **符号**                     | **描述**               | **维度**              | **说明**                                                                                                                                                                                        |
| -------------------------- | -------------------- | ------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| $\alpha$                   | 基础学习率（Learning Rate） | 标量                  | 决定每次参数更新的步长大小，在参数更新公式$\theta_{t + 1} = \theta_t - \alpha \cdot g_t$中，$\alpha$与梯度$g_t$相乘，控制参数$\theta$的更新幅度                                                                             |
| $\beta_1$                  | Adam 优化器一阶矩衰减率       | 标量                  | 用于计算移动平均梯度。在公式$m_t = \beta_1 m_{t - 1} + (1 - \beta_1)g_t$中，$m_t$表示$t$时刻的移动平均梯度，$\beta_1$决定了历史梯度信息在当前移动平均梯度计算中的权重，默认值为 0.9                                                            |
| $\beta_2$                  | Adam 优化器二阶矩衰减率       | 标量                  | 用于计算梯度平方的移动平均。在公式$v_t = \beta_2 v_{t - 1} + (1 - \beta_2)g_t^2$中，$v_t$表示$t$时刻梯度平方的移动平均，$\beta_2$决定了历史梯度平方信息在当前计算中的权重，默认值为 0.999                                                       |
| $\epsilon$                 | Adam 优化器数值稳定项        | 标量                  | 防止在参数更新过程中出现除零错误。在公式$\theta_{t + 1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$中，$\epsilon$通常设置为一个极小值，如 1e - 8                                           |
| $\lambda$                  | 权重衰减系数（Weight Decay） | 标量                  | 用于 L2 正则化，控制模型复杂度。在参数更新公式$\theta_{t + 1} = \theta_t - \alpha \cdot (g_t + \lambda \theta_t)$中，$\lambda$与参数$\theta_t$相乘后加到梯度上，使参数更新时向零收缩，避免过拟合                                         |
| $\gamma_{\text{momentum}}$ | SGD 动量系数             | 标量                  | 在传统随机梯度下降（SGD）带动量的更新中起作用。公式$v_t = \gamma v_{t - 1} + g_t$中，$v_t$为当前时刻的动量，$\gamma$决定了上一时刻动量在当前动量计算中的权重；在参数更新公式$\theta_{t + 1} = \theta_t - \alpha v_t$中，动量$v_t$影响参数$\theta$的更新 |
| $m_t$                      | Adam 优化器一阶矩估计值       | 向量（与参数$\theta$维度相同） | 是对梯度的移动平均估计，如公式$m_t = \beta_1 m_{t - 1} + (1 - \beta_1)g_t$所示，它综合考虑了历史梯度信息，帮助优化器在训练过程中更稳定地更新参数                                                                                        |
| $v_t$                      | Adam 优化器二阶矩估计值       | 向量（与参数$\theta$维度相同） | 是对梯度平方的移动平均估计，通过公式$v_t = \beta_2 v_{t - 1} + (1 - \beta_2)g_t^2$计算得到，用于自适应调整学习率                                                                                                       |
| $\hat{m}_t$                | Adam 优化器修正后的一阶矩估计值   | 向量（与参数$\theta$维度相同） | 通过公式$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}$计算得到，用于修正由于初始阶段$\beta_1$的累积效应导致的偏差，使移动平均梯度估计更准确                                                                                            |
| $\hat{v}_t$                | Adam 优化器修正后的二阶矩估计值   | 向量（与参数$\theta$维度相同） | 通过公式$\hat{v}_t = \frac{v_t}{1 - \beta_2^t}$计算得到，用于修正由于初始阶段$\beta_2$的累积效应导致的偏差，用于参数更新公式中以更准确地调整学习率                                                                                     |

#### **二、正则化参数**



| **符号**                 | **描述**                    | **维度** | **说明**                                                                                                                         |
| ---------------------- | ------------------------- | ------ | ------------------------------------------------------------------------------------------------------------------------------ |
| $p_{\text{drop}}$      | Dropout 丢弃概率              | 标量     | 在 Dropout 操作中，以该概率随机将激活值置零，如$x_{\text{drop}} = \text{Dropout}(x, p_{\text{drop}})$，通过随机失活神经元来防止过拟合                     |
| $\epsilon_{\text{ls}}$ | 标签平滑系数（Label Smoothing）   | 标量     | 用于软化目标分布。将真实标签概率设为 \$1 - \epsilon\_{\text {ls}} + \epsilon\_{\text {ls}} /                                                     |
| $\tau$                 | 梯度裁剪阈值（Gradient Clipping） | 标量     | 用于限制梯度范数。在公式$g_{\text{clip}} = g \cdot \min\left(1, \frac{\tau}{\|g\|}\right)$中，当梯度范数$\|g\|$超过阈值$\tau$时，对梯度进行缩放，防止梯度爆炸 |

#### **三、学习率调度参数**



| **符号**                | **描述**      | **维度** | **说明**                                                                                                                                                                                            |
| --------------------- | ----------- | ------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| $T_{\text{warmup}}$   | 学习率预热步数     | 标量     | 在训练初始阶段，学习率会在这$T_{\text{warmup}}$步内线性增加至$\alpha$，如公式$\alpha_t = \alpha \cdot \min\left(1, \frac{t}{T_{\text{warmup}}}\right) \quad (0 < t \leq T_{\text{warmup}})$所示，这样可以避免训练早期学习率过大导致不稳定 |
| $\alpha_{\text{max}}$ | 最大学习率（余弦退火） | 标量     | 在学习率调度周期内的峰值学习率，在余弦退火调度公式$\alpha_t = \alpha_{\text{min}} + \frac{1}{2}(\alpha_{\text{max}} - \alpha_{\text{min}})(1 + \cos(\pi t/T))$中，决定了学习率变化范围的上限                                      |
| $\alpha_{\text{min}}$ | 最小学习率（余弦退火） | 标量     | 在学习率调度周期内的谷值学习率，在余弦退火调度公式中，决定了学习率变化范围的下限                                                                                                                                                          |
| $T_{\text{cycle}}$    | 学习率调度周期长度   | 标量     | 定义了余弦退火或其他循环学习率策略的周期步数，在一个周期内学习率按照特定规律变化                                                                                                                                                          |

#### **四、训练配置参数**



| **符号**             | **描述**           | **维度** | **说明**                                                                     |
| ------------------ | ---------------- | ------ | -------------------------------------------------------------------------- |
| $B$                | 批量大小（Batch Size） | 标量     | 表示单次参数更新所使用的样本数量，影响模型训练的稳定性和效率                                             |
| $T_{\text{total}}$ | 总训练步数            | 标量     | 整个训练过程中模型参数更新的总次数                                                          |
| $N_{\text{epoch}}$ | 训练轮数（Epoch）      | 标量     | 完整遍历数据集的次数，通过公式 \$N\_{\text {epoch}} = \lceil T\_{\text {total}} \cdot B / |

#### **五、参数更新公式示例（Adam 优化器）涉及符号**



| **符号**           | **描述**         | **维度**              | **说明**                                                                                                               |
| ---------------- | -------------- | ------------------- | -------------------------------------------------------------------------------------------------------------------- |
| $g_t$            | $t$时刻的梯度       | 向量（与参数$\theta$维度相同） | 通过对损失函数$\mathcal{L}(\theta_t)$关于参数$\theta_t$求梯度得到，即$g_t = \nabla_{\theta} \mathcal{L}(\theta_t)$，它指示了参数更新的方向 |
| $\theta_t$       | $t$时刻的模型参数     | 向量（模型参数维度）          | 模型在$t$时刻的参数值，通过优化器不断更新，以最小化损失函数                                                                                      |
| $\theta_{t + 1}$ | $t + 1$时刻的模型参数 | 向量（模型参数维度）          | 根据优化器的更新规则，基于$t$时刻的参数$\theta_t$和相关梯度信息计算得到的下一时刻参数值                                                                   |

#### **六、参数初始化与训练流程涉及符号**



| **符号**                      | **描述**              | **维度**              | **说明**                                                                                                             |
| --------------------------- | ------------------- | ------------------- | ------------------------------------------------------------------------------------------------------------------ |
| $X$                         | 输入数据样本              | 张量（根据数据结构确定维度）      | 在训练过程中，从数据集中采样得到的输入数据，用于前向传播计算模型预测值                                                                                |
| $Y$                         | 真实标签                | 张量（根据数据结构确定维度）      | 与输入数据$X$对应的真实标签，用于计算损失函数                                                                                           |
| $Y_{pred}$                  | 模型预测值               | 张量（根据模型输出结构确定维度）    | 模型通过对输入$X$进行前向传播得到的预测结果                                                                                            |
| $\text{loss}$               | 损失值                 | 标量                  | 综合考虑模型预测值$Y_{pred}$与真实标签$Y$的差异，以及可能的正则化项（如$\lambda * L2\_norm(\theta)$）计算得到的损失，用于衡量模型当前的性能                         |
| $\text{gradients}$          | 计算得到的梯度             | 向量（与参数$\theta$维度相同） | 通过对损失函数关于参数$\theta$进行反向传播计算得到，即$\text{gradients} = \text{compute_gradients}(\text{loss}, \theta)$，用于指导参数更新 |
| $m\_prev$                   | 上一时刻 Adam 优化器一阶矩估计值 | 向量（与参数$\theta$维度相同） | 在 Adam 优化器参数更新过程中，用于计算当前时刻一阶矩估计值$m$的上一时刻值                                                                          |
| $v\_prev$                   | 上一时刻 Adam 优化器二阶矩估计值 | 向量（与参数$\theta$维度相同） | 在 Adam 优化器参数更新过程中，用于计算当前时刻二阶矩估计值$v$的上一时刻值                                                                          |
| $t$                         | 当前训练步数              | 标量                  | 记录训练过程中的步数，从 1 开始递增，直到达到总训练步数$T_{\text{total}}$                                                                    |
| $\text{Data}$               | 整个数据集               | -                   | 包含所有训练样本及其对应标签的集合，用于训练模型                                                                                           |
| $\text{clip\_by\_norm}$     | 梯度裁剪函数              | -                   | 实现梯度裁剪操作，将梯度$\text{gradients}$按照阈值$\tau$进行裁剪，返回裁剪后的梯度                                                              |
| $\text{compute\_gradients}$ | 计算梯度函数              | -                   | 根据损失函数$\text{loss}$和模型参数$\theta$计算梯度的函数                                                                            |
| $\text{cross\_entropy}$     | 交叉熵损失函数             | -                   | 用于计算模型预测值$Y_{pred}$与真实标签$Y$之间差异的损失函数                                                                               |
| $L2\_norm$                  | L2 范数计算函数           | -                   | 计算参数$\theta$的 L2 范数，用于权重衰减（L2 正则化）项的计算                                                                             |
| $\text{sqrt}$               | 平方根计算函数             | -                   | 在 Adam 优化器参数更新公式中，用于计算$\sqrt{\hat{v}_t}$                                                                   |
| $\text{min}$                | 取最小值函数              | -                   | 在学习率预热公式和梯度裁剪公式中，用于取两个值中的较小值                                                                                       |
| $\cos$                      | 余弦函数                | -                   | 在余弦退火学习率调度公式中，用于计算学习率随时间的变化                                                                                        |
| $\pi$                       | 圆周率                 | 标量                  | 数学常数，在余弦退火学习率调度公式中用于计算                                                                                             |
| $\lceil \cdot \rceil$       | 向上取整函数              | -                   | 在计算训练轮数$N_{\text{epoch}}$时，用于对 \$T\_{\text {total}} \cdot B /                                                      |
| \$                          | \cdot               | \$                  | 求数据集样本数函数                                                                                                          |
| $\text{Vocab}$              | 词汇表                 | -                   | 在标签平滑中，用于计算标签平滑后的概率分布，\$                                                                                           |
| $\text{Dropout}$            | Dropout 操作函数        | -                   | 实现 Dropout 正则化，按照概率$p_{\text{drop}}$随机将输入激活值置零                                                                     |
