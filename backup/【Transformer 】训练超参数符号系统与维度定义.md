

***

### **Transformer 训练超参数符号系统与维度定义**



***

#### **一、优化器参数**



| **符号**                     | **描述**               | **维度** | **数学定义 / 作用**                                                                                                  |
| -------------------------- | -------------------- | ------ | -------------------------------------------------------------------------------------------------------------- |
| $\alpha$                   | 基础学习率（Learning Rate） | 标量     | 参数更新步长，如 $\theta_{t + 1} = \theta_t - \alpha \cdot g_t$                                                        |
| $\beta_1$                  | Adam 优化器一阶矩衰减率       | 标量     | 动量衰减系数，默认 0.9，用于计算移动平均梯度 $m_t = \beta_1 m_{t - 1} + (1 - \beta_1)g_t$                                          |
| $\beta_2$                  | Adam 优化器二阶矩衰减率       | 标量     | 自适应学习率衰减系数，默认 0.999，计算梯度平方移动平均 $v_t = \beta_2 v_{t - 1} + (1 - \beta_2)g_t^2$                                  |
| $\epsilon$                 | Adam 优化器数值稳定项        | 标量     | 防止除零，默认 1e - 8，用于参数更新 $\theta_{t + 1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$ |
| $\lambda$                  | 权重衰减系数（Weight Decay） | 标量     | L2 正则化强度，更新公式 $\theta_{t + 1} = \theta_t - \alpha \cdot (g_t + \lambda \theta_t)$                              |
| $\gamma_{\text{momentum}}$ | SGD 动量系数             | 标量     | 传统动量项，更新公式 $v_t = \gamma v_{t - 1} + g_t$, $\theta_{t + 1} = \theta_t - \alpha v_t$                            |



***

#### **二、正则化参数**



| **符号**                 | **描述**                    | **维度** | **数学定义 / 作用**                                                             |
| ---------------------- | ------------------------- | ------ | ------------------------------------------------------------------------- |
| $p_{\text{drop}}$      | Dropout 丢弃概率              | 标量     | 激活值按概率置零，如 $x_{\text{drop}} = \text{Dropout}(x, p_{\text{drop}})$         |
| $\epsilon_{\text{ls}}$ | 标签平滑系数（Label Smoothing）   | 标量     | 软化目标分布，将真实标签概率设为 \$1 - \epsilon\_{\text {ls}} + \epsilon\_{\text {ls}} /  |
| $\tau$                 | 梯度裁剪阈值（Gradient Clipping） | 标量     | 限制梯度范数，$g_{\text{clip}} = g \cdot \min\left(1, \frac{\tau}{\|g\|}\right)$ |



***

#### **三、学习率调度参数**



| **符号**                | **描述**      | **维度** | **数学定义 / 作用**                                                                                                              |
| --------------------- | ----------- | ------ | -------------------------------------------------------------------------------------------------------------------------- |
| $T_{\text{warmup}}$   | 学习率预热步数     | 标量     | 初始阶段线性增加学习率至 $\alpha$，避免早期不稳定                                                                                              |
| $\alpha_{\text{max}}$ | 最大学习率（余弦退火） | 标量     | 调度周期内的峰值学习率，如 $\alpha_t = \alpha_{\text{min}} + \frac{1}{2}(\alpha_{\text{max}} - \alpha_{\text{min}})(1 + \cos(\pi t/T))$ |
| $\alpha_{\text{min}}$ | 最小学习率（余弦退火） | 标量     | 调度周期内的谷值学习率                                                                                                                |
| $T_{\text{cycle}}$    | 学习率调度周期长度   | 标量     | 余弦退火或循环策略的周期步数                                                                                                             |



***

#### **四、训练配置参数**



| **符号**             | **描述**           | **维度** | **数学定义 / 作用**                                                         |
| ------------------ | ---------------- | ------ | --------------------------------------------------------------------- |
| $B$                | 批量大小（Batch Size） | 标量     | 单次参数更新使用的样本数                                                          |
| $T_{\text{total}}$ | 总训练步数            | 标量     | 模型参数更新的总次数                                                            |
| $N_{\text{epoch}}$ | 训练轮数（Epoch）      | 标量     | 完整遍历数据集的次数，\$N\_{\text {epoch}} = \lceil T\_{\text {total}} \cdot B / |



***

#### **五、参数更新公式示例（Adam 优化器）**

**计算梯度**：

$g_t = \nabla_{\theta} \mathcal{L}(\theta_t)$

**更新一阶矩估计**：

 $= \beta_1 m_{t - 1} + (1 - \beta_1) g_t$

**更新二阶矩估计**：

  $v_t = \beta_2 v_{t - 1} + (1 - \beta_2) g_t^2$

**修正偏差**：

$\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$

**参数更新**：

$\theta_{t + 1} = \theta_t - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} - \alpha \lambda \theta_t$



***

#### **六、参数初始化与训练流程**

**初始化优化器状态**：

$m_0 = 0$, $v_0 = 0$（Adam 初始状态）

学习率按预热策略调整：

$\alpha_t = \alpha \cdot \min\left(1, \frac{t}{T_{\text{warmup}}}\right) \quad (0 < t \leq T_{\text{warmup}})$

**单步训练伪代码**：



```
for t in 1...T\_total:

&#x20;   \# 1. 前向传播与损失计算

&#x20;   X, Y = sample\_batch(data, B)

&#x20;   Y\_pred = transformer(X)

&#x20;   loss = cross\_entropy(Y\_pred, Y) + λ \* L2\_norm(θ)

&#x20;  &#x20;

&#x20;   \# 2. 反向传播

&#x20;   gradients = compute\_gradients(loss, θ)

&#x20;  &#x20;

&#x20;   \# 3. 梯度裁剪

&#x20;   gradients = clip\_by\_norm(gradients, τ)

&#x20;  &#x20;

&#x20;   \# 4. 更新参数（Adam）

&#x20;   m = β1 \* m\_prev + (1 - β1) \* gradients

&#x20;   v = β2 \* v\_prev + (1 - β2) \* gradients\*\*2

&#x20;   m\_hat = m / (1 - β1\*\*t)

&#x20;   v\_hat = v / (1 - β2\*\*t)

&#x20;   θ = θ - α\_t \* (m\_hat / (sqrt(v\_hat) + ε) + λ \* θ)
```



***

### **总结**

此符号系统完整覆盖了 Transformer 模型的 **可调模型参数**（如权重矩阵、归一化参数）与 **训练超参数**（如学习率、优化器配置、正则化项），并通过数学公式与伪代码明确了各参数在训练流程中的作用。关键设计包括：

**模型 - 训练解耦**：区分架构参数（$W, \gamma, \beta$）与训练参数（$\alpha, \beta_1, \lambda$）。

**动态学习率**：通过预热、余弦退火等策略实现精细控制。

**正则化协同**：组合使用 Dropout、标签平滑与梯度裁剪提升泛化性。

此框架为理论分析、超参数调优及代码实现提供了完备的数学参考。